---
permalink: /
title: " "
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<span style="color: rgba(0,0,128,0.9);">This course *Optimization Methods* is oreinted for graduate students from applied statistics at the department of mathematics, Jinan University. </span>

 <a href="https://wangyuanhao.github.io" style="text-decoration:none;color:purple">**Instructor: Weiwen Wang(çŽ‹ä¼Ÿæ–‡)**</a>

The materials are collected and reorganized mainly from:

* Beck, A. *Introduction to Non-linear Optimization: Theory, Algorithm, and Applications in MALTAB*, SIAM, 2014.
* Beck, A. *First-Order Methods in Optimization*, SIAM,  2017.
* Gartner, B., He, N., and Jaggi, M. Lectures notes on *Optimization for Data Science*.
* Nesterov, Y. *Lectures on Convex Optimization*, Springer, 2018
* Nesterov, Y. Lecture notes on *Modern Optimization* in 2024 summer school at Peking University.
* Lan, G. *First-order and Stochastic Optimization Methods for Machine Learning*, Springer, 2020.

<span style="color: rgba(0,0,128,0.9);">Students must read at least one paper listed below that is most attractive to you in each section. </span>

<span style="color: rgba(90,90, 90,0.5);">Syllabus</span>
======
### ðŸš© <span style="color: rgba(0,0, 205,0.9);">Convex And Smooth Functions</span>

### ðŸš© <span style="color: rgba(0,0, 205,0.9);">Gradient Descent</span>

### ðŸš© <span style="color: rgba(0,0, 205,0.9);">Coordinate Descent and Conjugate Gradient Descent</span>

* [Shewchuk, J. R. (1994). An introduction to the conjugate gradient method without the agonizing pain.](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)
* [Nocedal, J., & Wright, S. J. (Eds.). (2006). *Numerical optimization*. 2nd Edition,  New York, NY: Springer New York. Chapter 5.](https://link.springer.com/book/10.1007/978-0-387-40065-5)

### ðŸš© <span style="color: rgba(0,0, 205,0.9);">Projected Gradient Descent</span>

### ðŸš© <span style="color: rgba(0,0,205,0.9);">Introductory to Computational Complexity</span>

### ðŸš© <span style="color: rgba(0,0,205,0.9);">Conjugate Functions</span>

### ðŸš© <span style="color: rgba(0,0,205,0.9);">Proximal Operator</span>

### ðŸš© <span style="color: rgba(0,0,205,0.9);">Mirror Descent</span>

### ðŸš© <span style="color: rgba(0,0,205,0.9);">Frank-Wolfe Algorithm</span>

* [Jaggi, Martin. "Revisiting Frank-Wolfe: Projection-free sparse convex optimization." *International conference on machine learning*. PMLR, 2013.](http://proceedings.mlr.press/v28/jaggi13.pdf)
* [Ding, Lijun, et al. "Spectral frank-wolfe algorithm: Strict complementarity and linear convergence." *International conference on machine learning*. PMLR, 2020.](http://proceedings.mlr.press/v119/ding20a/ding20a.pdf)
* [Dvurechensky, Pavel, et al. "Self-concordant analysis of Frank-Wolfe algorithms." *International Conference on Machine Learning*. PMLR, 2020.](http://proceedings.mlr.press/v119/dvurechensky20a/dvurechensky20a.pdf)
* [Zhou, Baojian, and Yifan Sun. "Approximate Frank-Wolfe Algorithms over Graph-structured Support Sets." *International Conference on Machine Learning*. PMLR, 2022.](https://proceedings.mlr.press/v162/zhou22i/zhou22i.pdf)

### ðŸš© <span style="color: rgba(0,0,205,0.9);">Stochastic Optimization</span>

### ðŸš© <span style="color: rgba(0,0,205,0.9);">Non-smooth Convex Optimization</span>

### ðŸš© <span style="color: rgba(0,0,205,0.9);">Multi-objective Optimization and Its Applications on Multi-task Learning</span>

* [Pardalos, P.M.,  Å½ilinskas, A.,  Å½ilinskas, J.  *Non-Convex Multi-Objective Optimization*, Chapter 1-2, Springer, 2017.](https://link.springer.com/book/10.1007/978-3-319-61007-8) 

<span style="color: rgba(90,90, 90,0.5);">History</span>
======
* <span style="color: rgba(0,0,128,0.9);">[2024-09-21] Create this webpage.</span>

  
