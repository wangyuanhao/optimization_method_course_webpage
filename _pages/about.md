---
permalink: /
title: " "
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<span style="color: rgba(0,0,128,0.9);">This course *Optimization Methods* is oreinted for graduate students from applied statistics at the department of mathematics, Jinan University. The materials are collected and reorganized mainly from:</span>

* Beck, A. *Introduction to Non-linear Optimization: Theory, Algorithm, and Applications in MALTAB*, SIAM, 2014.
* Beck, A. *First-Order Methods in Optimization*, SIAM,  2017.
* Gartner, B., He, N., and Jaggi, M. Lectures notes on *Optimization for Data Science*.
* Nesterov, Y. *Lectures on Convex Optimization*, Springer, 2018
* Nesterov, Y. Lecture notes on *Modern Optimization* in 2024 summer school at Peking University.
* Lan, G. *First-order and Stochastic Optimization Methods for Machine Learning*, Springer, 2020.

<span style="color: rgba(0,0,128,0.9);">Students must read at least one paper listed below that is most attractive to you in each section. </span>

<span style="color: rgba(0,0, 205,0.8);">Syllabus</span>
======
### ðŸš© <span style="color: rgba(0,0,128,0.7);">Convex And Smooth Functions</span>

### ðŸš© <span style="color: rgba(0,0,128,0.7);">Gradient Descent</span>

### ðŸš© <span style="color: rgba(0,0,128,0.7);">Projected Gradient Descent</span>

### ðŸš© <span style="color: rgba(0,0,128,0.7);">Introductory to Computational Complexity</span>

### ðŸš© <span style="color: rgba(0,0,128,0.7);">Conjugate Functions</span>

### ðŸš© <span style="color: rgba(0,0,128,0.7);">Proximal Operator</span>

### ðŸš© <span style="color: rgba(0,0,128,0.7);">Mirror Descent</span>

### ðŸš© <span style="color: rgba(0,0,128,0.7);">Frank-Wolfe Algorithm</span>

* [Jaggi, Martin. "Revisiting Frank-Wolfe: Projection-free sparse convex optimization." *International conference on machine learning*. PMLR, 2013.](http://proceedings.mlr.press/v28/jaggi13.pdf)
* [Ding, Lijun, et al. "Spectral frank-wolfe algorithm: Strict complementarity and linear convergence." *International conference on machine learning*. PMLR, 2020.](http://proceedings.mlr.press/v119/ding20a/ding20a.pdf)
* [Dvurechensky, Pavel, et al. "Self-concordant analysis of Frank-Wolfe algorithms." *International Conference on Machine Learning*. PMLR, 2020.](http://proceedings.mlr.press/v119/dvurechensky20a/dvurechensky20a.pdf)
* [Zhou, Baojian, and Yifan Sun. "Approximate Frank-Wolfe Algorithms over Graph-structured Support Sets." *International Conference on Machine Learning*. PMLR, 2022.](https://proceedings.mlr.press/v162/zhou22i/zhou22i.pdf)

### ðŸš© <span style="color: rgba(0,0,128,0.7);">Stochastic Optimization</span>

### ðŸš© <span style="color: rgba(0,0,128,0.7);">Non-smooth Convex Optimization</span>

### ðŸš© <span style="color: rgba(0,0,128,0.7);">Multi-objective Optimization and Its Applications on Multi-task Learning</span>

* [Pardalos, P.M.,  Å½ilinskas, A.,  Å½ilinskas, J.  *Non-Convex Multi-Objective Optimization*, Chapter 1-2, Springer, 2017.](https://link.springer.com/book/10.1007/978-3-319-61007-8) 

<span style="color: rgba(0,0, 205,0.8);">History</span>
======
* <span style="color: rgba(139,0,139,0.8);">[2024-09-21] Create this webpage.</span>

  
